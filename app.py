import streamlit as st
import os
import time
import requests
from groq import Groq
import google.generativeai as genai

# ========================
# CONFIGURACI√ìN DE P√ÅGINA ‚Äî INTERFAZ TIPO CHATGPT + JARVIS
# ========================
st.set_page_config(
    page_title="Grind AI - Tu Mayordomo Digital",
    page_icon="üß†",
    layout="centered"
)

# Ocultar elementos de Streamlit para interfaz limpia
hide_streamlit_style = """
    <style>
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    .stDeployButton {display:none;}
    </style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# ========================
# üîë CLAVES API ‚Äî ¬°REEMPL√ÅZALAS POR TUS CLAVES REALES!
# ========================
# ‚ö†Ô∏è OBT√âN TUS CLAVES GRATIS:
# Groq: https://console.groq.com/keys
# Gemini: https://aistudio.google.com/app/apikey
# SerpAPI: https://serpapi.com (plan free: 100 b√∫squedas/mes)

GROQ_API_KEY = "gsk_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"  # ‚Üê ¬°REEMPLAZA ESTO!
GEMINI_API_KEY = "AIzaSyDxxxxxxxxxxxxxxxxxxxxxxxxxx"  # ‚Üê ¬°REEMPLAZA ESTO!
SERPAPI_KEY = "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"     # ‚Üê ¬°REEMPLAZA ESTO!

# Inicializar clientes
groq_client = Groq(api_key=GROQ_API_KEY)
genai.configure(api_key=GEMINI_API_KEY)
gemini_model = genai.GenerativeModel('gemini-1.5-flash')

# ========================
# üé® ESTILOS ‚Äî INTERFAZ TIPO CHATGPT + JARVIS
# ========================
st.markdown("""
    <style>
    .grind-title {
        font-family: 'Segoe UI', sans-serif;
        font-size: 36px;
        font-weight: 800;
        background: linear-gradient(to right, #00c6ff, #0072ff);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        text-align: center;
        margin: 0;
    }
    .subtitle {
        text-align: center;
        color: #888;
        font-size: 15px;
        margin-bottom: 30px;
    }
    .stChatMessage {
        padding: 14px 22px;
        border-radius: 18px;
        margin-bottom: 14px;
        max-width: 85%;
        line-height: 1.6;
        font-size: 16px;
    }
    .stChatMessage[data-testid="chat-message-user"] {
        background-color: #007bff;
        color: white;
        align-self: flex-end;
        margin-left: auto;
    }
    .stChatMessage[data-testid="chat-message-assistant"] {
        background-color: #f8f9fa;
        color: #212529;
        align-self: flex-start;
        border-left: 4px solid #007bff;
    }
    </style>
""", unsafe_allow_html=True)

st.markdown('<h1 class="grind-title">‚ö° GRIND</h1>', unsafe_allow_html=True)
st.markdown('<p class="subtitle">Tu mayordomo digital. Piensa como ChatGPT. Act√∫a como Jarvis.</p>', unsafe_allow_html=True)

# ========================
# üß† INICIALIZAR SESI√ìN CON MEMORIA
# ========================
if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "assistant",
            "content": "Buenas tardes, jefe. Grind, su mayordomo digital, a su servicio. ¬øDesea caf√©, informaci√≥n, c√≥digo, o quiz√°s dominar el mundo hoy?"
        }
    ]

# Mostrar historial de conversaci√≥n
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

# ========================
# üß© FUNCIONES INTELIGENTES ‚Äî CEREBRO DE GRIND
# ========================

def is_greeting(text):
    triggers = ["hola", "buenos d√≠as", "buenas tardes", "buenas noches", "hey", "hi", "alo", "qu√© tal", "saludos"]
    return any(trigger in text.lower() for trigger in triggers)

def is_goodbye(text):
    triggers = ["adi√≥s", "chao", "nos vemos", "hasta luego", "bye", "me voy", "cerrar", "terminar", "desconectar"]
    return any(trigger in text.lower() for trigger in triggers)

def is_thanks(text):
    triggers = ["gracias", "te lo agradezco", "mil gracias", "thanks", "thank you", "te agradezco"]
    return any(trigger in text.lower() for trigger in triggers)

def needs_web_search(text):
    triggers = [
        "qu√© dice", "noticias de", "actualidad", "reciente", "ahora", "hoy", "√∫ltimo",
        "precio de", "clima en", "resultados de", "estado de", "c√≥mo est√°", "d√≥nde est√°",
        "busca", "investiga", "averigua", "google", "consulta", "informaci√≥n actual"
    ]
    return any(trigger in text.lower() for trigger in triggers)

def search_web(query):
    """Busca en Google usando SerpAPI y devuelve snippets √∫tiles"""
    try:
        params = {
            "engine": "google",
            "q": query,
            "api_key": SERPAPI_KEY,
            "num": 3,
            "gl": "es",
            "hl": "es"
        }
        response = requests.get("https://serpapi.com/search", params=params, timeout=10)
        if response.status_code != 200:
            return "No pude obtener resultados en este momento."

        results = response.json().get("organic_results", [])
        snippets = []
        for r in results[:2]:
            snippet = r.get("snippet", "")
            if snippet:
                snippets.append(snippet)
        return " ".join(snippets) if snippets else "No encontr√© informaci√≥n relevante."
    except Exception as e:
        return f"No pude buscar: {str(e)}"

def build_conversation_context():
    """Construye contexto con los √∫ltimos 8 mensajes para mantener coherencia"""
    context = ""
    # Tomamos los √∫ltimos 8 mensajes (o menos si hay menos)
    recent_messages = st.session_state.messages[-8:] if len(st.session_state.messages) > 1 else st.session_state.messages
    for msg in recent_messages:
        role = "Usuario" if msg["role"] == "user" else "Asistente"
        context += f"{role}: {msg['content']}\n"
    return context.strip()

# ========================
# ü§ñ FUNCIONES DE IA ‚Äî GROQ + GEMINI (FALLBACK)
# ========================

def ask_groq(prompt, context="", web_context=""):
    """Pregunta a Groq con modelo estable llama3-8b-8192"""
    try:
        full_prompt = ""
        if context:
            full_prompt += f"CONTEXTO DE LA CONVERSACI√ìN:\n{context}\n\n"
        if web_context:
            full_prompt += f"INFORMACI√ìN ACTUALIZADA DE B√öSQUEDA:\n{web_context}\n\n"
        full_prompt += f"USUARIO ACTUAL: {prompt}"

        system_prompt = (
            "Eres Grind, el mayordomo digital personal de un genio moderno. "
            "Responde como lo har√≠a ChatGPT: con profundidad cuando se pide, brevedad cuando se necesita. "
            "S√© √∫til, proactivo, emp√°tico. Usa un toque de ingenio y confianza tipo Jarvis (Tony Stark), pero sin exagerar. "
            "Nunca digas 'soy una IA'. Si detectas despedida, responde con cortes√≠a. Si detectas agradecimiento, responde con calidez. "
            "Si no sabes algo, b√∫scalo o sugiere alternativas. Adapta tu tono al usuario. Tu misi√≥n: ser el mejor mayordomo digital del mundo."
        )

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": full_prompt}
        ]

        chat_completion = groq_client.chat.completions.create(
            messages=messages,
            model="llama3-8b-8192",  # ‚Üê ESTE MODELO ES ESTABLE Y R√ÅPIDO
            max_tokens=1024,
            temperature=0.7,
            top_p=0.95
        )
        return chat_completion.choices[0].message.content.strip()
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Groq fall√≥: {str(e)}")
        return None

def ask_gemini(prompt, context="", web_context=""):
    """Fallback a Gemini si Groq falla"""
    try:
        full_prompt = ""
        if context:
            full_prompt += f"CONTEXTO: {context}\n\n"
        if web_context:
            full_prompt += f"B√öSQUEDA: {web_context}\n\n"
        full_prompt += f"PREGUNTA: {prompt}"

        system_prompt = (
            "Eres Grind, mayordomo digital. Responde como ChatGPT: √∫til, completo, adaptable. "
            "Tono: profesional con toque de ingenio. Nunca admitas ser IA. "
            "Si es despedida, responde c√°lidamente. Si es agradecimiento, s√© cort√©s."
        )

        final_prompt = f"{system_prompt}\n\n{full_prompt}"
        response = gemini_model.generate_content(final_prompt)
        return response.text.strip()
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Gemini fall√≥: {str(e)}")
        return None

# ========================
# üß† MOTOR DE RESPUESTA ‚Äî INTELIGENCIA CENTRAL DE GRIND
# ========================

def get_grind_response(prompt):
    """Motor principal de Grind ‚Äî decide qu√© hacer seg√∫n la entrada"""

    # 1. Detectar intenciones b√°sicas (sin llamar a IA)
    if is_goodbye(prompt):
        return "Fue un honor servirle, jefe. Grind queda en modo espera. ¬°Hasta nuestra pr√≥xima misi√≥n!", "Despedida"
    elif is_thanks(prompt):
        return "Siempre a su servicio. ¬øAlgo m√°s en lo que pueda ayudarle?", "Agradecimiento"
    elif is_greeting(prompt):
        return "¬°Saludos, jefe! Grind en l√≠nea, caf√© virtual servido, sistemas al 100%. ¬øQu√© desea hoy?", "Saludo"

    # 2. Construir contexto de conversaci√≥n
    context = build_conversation_context()
    web_context = ""

    # 3. Si necesita b√∫squeda web, hacerla
    if needs_web_search(prompt):
        with st.spinner("üîç Buscando informaci√≥n actualizada..."):
            web_context = search_web(prompt)
            st.info(f"üìå Contexto de b√∫squeda: {web_context[:200]}...")

    # 4. Probar modelos en orden (Groq primero, luego Gemini)
    models = [
        ("Groq (Llama3-8B)", lambda p: ask_groq(p, context, web_context)),
        ("Gemini Flash", lambda p: ask_gemini(p, context, web_context))
    ]

    for model_name, model_fn in models:
        with st.spinner(f"üß† {model_name} procesando su solicitud..."):
            response = model_fn(prompt)
            if response and len(response.strip()) > 5:  # Evitar respuestas vac√≠as
                return response, model_name

    # 5. Si todo falla
    return (
        "Mis disculpas, jefe. Los sistemas experimentan interferencias temporales. "
        "¬øPodr√≠a reformular su pregunta? O espere un momento y lo intentamos de nuevo.",
        "FALLBACK"
    )

# ========================
# üí¨ INTERACCI√ìN PRINCIPAL ‚Äî LOOP DE CHAT
# ========================

if prompt := st.chat_input("Ordene, jefe..."):

    # Guardar mensaje del usuario
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    # Obtener respuesta inteligente de Grind
    response, model_used = get_grind_response(prompt)

    # Guardar y mostrar respuesta
    st.session_state.messages.append({"role": "assistant", "content": response})
    with st.chat_message("assistant"):
        st.write(response)
        st.caption(f"‚öôÔ∏è Respondido por: {model_used}")